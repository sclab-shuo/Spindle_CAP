{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< folder exists > : /Volumes/JDlab_Shuo/CoRe/Spindle_CAP/withinSBJ_CoRe_022\n",
      " \n",
      " \n",
      "  >>  Loading data on subject:  CoRe_100\n",
      " \n",
      " \n",
      "['/Volumes/JDlab_Shuo/CoRe/MRI_PreP/CoRe_100/D1/11-sleep/CoRe_100_D1_sleep_MDsCoNr%mniNLS.nii.gz']\n",
      "(547, 3)\n",
      "3369\n",
      "(546, 3)\n",
      "(458,)\n",
      "72 115 123 124 143 154 174 183 197 198 199 200 202 204 205 209 219 229 234 240 241 242 253 254 257 259 261 264 275 276 284 288 290 291 294 300 301 303 305 313 316 321 322 324 325 327 330 331 341 349 354 355 372 376 377 378 380 384 389 392 395 397 398 399 412 416 417 418 420 427 428 429 431 433 434 435 437 438 439 440 449 451 455 463 465 467 468 469 476 477 494 498 505 506 507 508 509 519 531 541 542 556 558 569 580 584 585 587 588 589 590 593 595 605 606 609 618 619 620 621 622 623 647 648 650 656 667 669 672 675 701 702 708 723 725 733 735 736 741 742 743 745 746 747 752 765 766 767 781 783 787 797 799 802 810 815 827 857 861 867 871 873 876 880 883 888 897 903 911 925 932 940 973 988 999 1015 1034 1037 1061 1070 1106 1122 1130 1140 1158 1169 1178 1182 1218 1282 1294 1312 1330 1338 1340 1352 1374 1423 1487 1500 1501 1513 1554 1556 1642 1645 1649 1655 1765 1784 1785 1786 1787 1788 1806 1807 1816 1817 1818 1830 1844 1854 1864 1866 1875 1876 1877 1879 1897 1898 1899 1900 1910 1914 1924 1925 1928 1929 1941 1942 1943 1944 1945 1946 1955 1957 1969 1971 1976 1986 1990 1991 1992 1994 1995 2018 2019 2020 2021 2023 2030 2037 2038 2039 2042 2050 2051 2052 2053 2069 2071 2085 2090 2091 2092 2093 2094 2100 2106 2119 2121 2124 2131 2133 2138 2234 2237 2292 2294 2295 2474 2475 2478 2479 2489 2490 2491 2492 2536 2537 2538 2539 2551 2554 2557 2568 2572 2574 2575 2579 2580 2581 2586 2591 2601 2602 2616 2617 2618 2620 2623 2638 2639 2640 2641 2656 2662 2673 2704 2705 2706 2707 2728 2729 2730 2732 2734 2737 2747 2749 2752 2754 2755 2756 2757 2758 2766 2769 2776 2777 2778 2779 2781 2787 2788 2789 2791 2793 2795 2802 2807 2808 2810 2816 2817 2819 2821 2823 2824 2826 2831 2840 2844 2846 2849 2854 2855 2870 2877 2878 2880 2893 2897 2899 2901 2904 2906 2908 2910 2911 2915 2920 2921 2922 2929 2930 2932 2934 2946 2964 2970 2971 2972 2978 2981 2987 2992 2997 2998 2999 3005 3006 3012 3013 3016 3023 3025 3026 3027 3030 3120 3121 3122 3170 3172 3175 3190 3191 3193 3194 3195 3196 3205 3207 3208 3209 3210 3211 3212 3213 3233 3252 3253 3255 3260 3261 3262 3274 3275 3279 3280 3338 3340 3341 3342 3352 3361 3367\n",
      "  - I am trying my best to save out the big matrix, be patient mate...\n",
      "  \n",
      "  - solving for 2 cluster plan...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clusterer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-225-8860ca6c751d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'  - solving for '\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' cluster plan...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0;32mdel\u001b[0m \u001b[0mclusterer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilhouette_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;31m# Create a subplot with 1 row and 2 columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clusterer' is not defined"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# todo: \n",
    "# 1. spatial_mask.inverse_transform, sometimes you dont have the spatial_mask store in the local() to use.\n",
    "# 2. clustering within sbj to see the trend, with selected large spindle-TR sample size sbj\n",
    "# 3. pca, limited number of eigenvector to feed in clustering, then be used to reconstruct data.\n",
    "# 4. a handle about whether save it, eg. SpindleTR_SampleSize_N11, into the folder; also when plot it, where direct to extract\n",
    "# 5. motion outliers exclusion? maybe not, check how many could be excluded because of motion\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob as gb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from nilearn.input_data import NiftiMasker\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score, pairwise_distances, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import LeaveOneGroupOut\n",
    "# from sklearn.feature_selection import SelectKBest, SelectPercentile, f_classif\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# sbjlist = [ \"CoRe_001\", \"CoRe_023\", \"CoRe_079\", \"CoRe_107\",\n",
    "# \"CoRe_082\", \"CoRe_100\", \"CoRe_087\", \"CoRe_192\", \"CoRe_195\",\n",
    "# \"CoRe_235\", \"CoRe_267\", \"CoRe_237\", \"CoRe_296\",\n",
    "# \"CoRe_011\", \"CoRe_022\", \"CoRe_050\", \"CoRe_054\", \"CoRe_162\", \"CoRe_155\", \"CoRe_094\"] \n",
    "# N=18 \"CoRe_063\", \"CoRe_128\", \"CoRe_220\", \"CoRe_223\", \"CoRe_268\", # N=8 \"CoRe_102\",\n",
    "\n",
    "# sbjlist = [ \"CoRe_079\",\n",
    "# \"CoRe_082\", \"CoRe_100\", \"CoRe_195\",\n",
    "# \"CoRe_235\", \"CoRe_267\", \"CoRe_237\",\n",
    "# \"CoRe_011\", \"CoRe_022\", \"CoRe_054\", \"CoRe_094\"] \n",
    "\n",
    "sbjlist = [ \"CoRe_100\"] #011\n",
    "\n",
    "special_sbjs = {'sbj_ID': ['CoRe_001'], \n",
    "        'fMRI_sess_2use': [0]\n",
    "        }\n",
    "\n",
    "ref_special = pd.DataFrame(special_sbjs, columns = ['sbj_ID', 'fMRI_sess_2use'])\n",
    "\n",
    "daylist = [\"D1\"]\n",
    "\n",
    "data_suffix = 'MDsCoNr%mniNLS.nii.gz'\n",
    "dir_root = '/Volumes/JDlab_Shuo/CoRe'\n",
    "\n",
    "# collect all the masks intended to be used\n",
    "masklist = []\n",
    "for dr in os.listdir(dir_root + '/Analysis_cabinet/ROIs_mask/'):\n",
    "    if dr == 'mnit1_09c_3mm_mask.nii': #\n",
    "        masklist.append(dr)\n",
    "mask_filename = dir_root + '/Analysis_cabinet/ROIs_mask/'+ masklist[0]       \n",
    "        \n",
    "# make a table to save out the data inclusion info        \n",
    "df_samples = pd.DataFrame(np.zeros((len(sbjlist), 4)),\n",
    "                   columns=['total_spindle', 'total_TR', 'total_sleep_sess', 'specified_sleep_sess'])\n",
    "\n",
    "df_samples['sbjlist']=sbjlist\n",
    "df_samples.specified_sleep_sess+=-1 # -1 means haven't needed to specified\n",
    "\n",
    "save_data_SZ = 1\n",
    "\n",
    "save_fmri_mat = 1\n",
    "load_fmri_mat = 0\n",
    "\n",
    "fmri_mat_name = dir_root + '/Spindle_CAP/fmri_all_selected_N' + str(len(sbjlist)) + '.csv'\n",
    "# fmri_mat_name = dir_root + '/Spindle_CAP/fmri_all_selected_N3.csv'\n",
    "\n",
    "x_existed = 0\n",
    "\n",
    "# pca_on = 1\n",
    "\n",
    "# newfolder =  dir_root + '/Spindle_CAP/N20_informed_N11'\n",
    "newfolder =  dir_root + '/Spindle_CAP/withinSBJ_CoRe_022'\n",
    "\n",
    "if os.path.isdir(newfolder):\n",
    "    print('< folder exists > : ' + newfolder)\n",
    "else:\n",
    "    os.mkdir(newfolder)\n",
    "            \n",
    "#=================== [ loop through subjects, concatenating the spindle-related TRs first ] ====================#\n",
    "\n",
    "if 'fmri_all_selected' in locals():\n",
    "    del fmri_all_selected\n",
    "\n",
    "if x_existed == 0 and load_fmri_mat !=1: # initiate the spindle-TR extraction step.\n",
    "    \n",
    "    for sbj in sbjlist:\n",
    "\n",
    "        print(' ')\n",
    "        print(' ')\n",
    "        print('  >>  Loading data on subject:  ' + sbj )\n",
    "        print(' ')\n",
    "        print(' ')\n",
    "\n",
    "        if 'fmri_masked' in locals():\n",
    "            del fmri_masked\n",
    "        if 'fmri_1sess_selected' in locals():\n",
    "            del fmri_1sess_selected\n",
    "        if 'fmri_1sbj_selected' in locals():\n",
    "            del fmri_1sbj_selected        \n",
    "\n",
    "\n",
    "\n",
    "        # select and store the fmri data file names\n",
    "        curr_sleep_fmri = []\n",
    "        for day in daylist:\n",
    "            for dr in os.listdir(dir_root + '/MRI_PreP/' + sbj + '/'+ day):\n",
    "                if dr[-5:] == 'sleep': #\n",
    "                    curr_sleep_fmri.append(day + '/' + dr)\n",
    "\n",
    "        # select and store the spindle mark file names\n",
    "        curr_spinlde_mark = []\n",
    "        for dr in os.listdir(dir_root + '/Spindles/CoRe_spindles/output/'):\n",
    "            if dr[0:8] == sbj and dr[-4:] == '.txt': #\n",
    "                curr_spinlde_mark.append(dr)       \n",
    "\n",
    "\n",
    "        # if numbers not matched, go check and delete based on the special sbj reference list.   \n",
    "        if len(curr_spinlde_mark) < len(curr_sleep_fmri):\n",
    "            print(curr_sleep_fmri)\n",
    "            print('  - oops, ' + sbj + ' has an inconsistent match of numbers of fmri data and spindle mark files')\n",
    "\n",
    "            if np.sum([ref_special['sbj_ID'] == sbj]) > 0: # means at least one row contains the info for this sbj\n",
    "                sess_selected = ref_special['fMRI_sess_2use'].values[ref_special['sbj_ID'] == sbj][0]\n",
    "                curr_sleep_fmri = [curr_sleep_fmri[sess_selected]]\n",
    "                print('  - personalized file selection : [ ' + curr_sleep_fmri[0] + ' ]')\n",
    "            else:\n",
    "                print('  - oops, ' + sbj + ' does not have special reference information recorded!')\n",
    "\n",
    "            if save_data_SZ == 1:                \n",
    "                df_samples.specified_sleep_sess.iloc[sbjlist.index(sbj)] = sess_selected\n",
    "\n",
    "\n",
    "        # load, filter, then concatenate fmri data    \n",
    "        if len(curr_spinlde_mark) == len(curr_sleep_fmri):\n",
    "\n",
    "            spindle_count = 0\n",
    "            for ss in range(len(curr_sleep_fmri)):\n",
    "\n",
    "                # load fmri data,\n",
    "                train_path = gb.glob(dir_root + '/MRI_PreP/' + sbj + '/' + curr_sleep_fmri[ss])\n",
    "                train_data = gb.glob(train_path[0] + '/*' + data_suffix) \n",
    "                print(train_data)\n",
    "\n",
    "                # Z-scoring is done in this step as \"standardize=True\",\n",
    "                spatial_mask = NiftiMasker(mask_img=mask_filename, standardize=True)\n",
    "                fmri_masked = spatial_mask.fit_transform(train_data[0])\n",
    "\n",
    "                # load spindle marks,\n",
    "                spindle_mark = pd.read_csv(gb.glob(dir_root + '/Spindles/CoRe_spindles/output/' + curr_spinlde_mark[ss])[0], delimiter=' ')\n",
    "\n",
    "                # filter the fmri by spindle marks; \"spindle_mark\"-1 since here 0=1,\n",
    "                print(spindle_mark.shape)\n",
    "                print(fmri_masked.shape[0])\n",
    "# here\n",
    "# not = before  spindle_mark = spindle_mark[spindle_mark.V_center < fmri_masked.shape[0]] # in case some marks are outside of fmri length\n",
    "                spindle_mark = spindle_mark[spindle_mark.V_center <= fmri_masked.shape[0]] # in case some marks are outside of fmri length\n",
    "                print(spindle_mark.shape)\n",
    "\n",
    "                spindle_mark = spindle_mark-1\n",
    "                spindle_count = spindle_count + spindle_mark.shape[0]\n",
    "\n",
    "                fmri_1sess_selected = fmri_masked[np.unique(spindle_mark.V_center)]\n",
    "# here\n",
    "\n",
    "                ttt= np.unique(spindle_mark.V_center)\n",
    "                print(ttt.shape)\n",
    "                print(*ttt)\n",
    "\n",
    "\n",
    "                # concatenate the selected fmri data within the sbj, eg core_022\n",
    "                if ss == 0:                             # means this is the 1st session inside this loop.\n",
    "                    fmri_1sbj_selected = fmri_1sess_selected\n",
    "                else:                                   # not the first, so combine other sessions into one.\n",
    "                    fmri_1sbj_selected = np.concatenate((fmri_1sbj_selected, fmri_1sess_selected), axis=0)\n",
    "\n",
    "\n",
    "            if save_data_SZ == 1:    \n",
    "                df_samples.total_spindle.iloc[sbjlist.index(sbj)] = spindle_count\n",
    "                df_samples.total_TR.iloc[sbjlist.index(sbj)] = fmri_1sbj_selected.shape[0]\n",
    "                df_samples.total_sleep_sess.iloc[sbjlist.index(sbj)] = len(curr_sleep_fmri)\n",
    "        else:\n",
    "            print('  - It is weird, ' + sbj + ' still has an inconsistent match')\n",
    "\n",
    "\n",
    "        # concatenate the selected fmri data across the sbj\n",
    "        if sbjlist.index(sbj) == 0:                    # means this is the 1st sbj inside this loop.\n",
    "            fmri_all_selected = fmri_1sbj_selected\n",
    "        else:                                          # not the first, so combine other sbjs into one.\n",
    "            fmri_all_selected = np.concatenate((fmri_all_selected, fmri_1sbj_selected), axis=0)\n",
    "\n",
    "    if save_fmri_mat == 1:  \n",
    "        print('  - I am trying my best to save out the big matrix, be patient mate...')\n",
    "        np.savetxt(fmri_mat_name, fmri_all_selected, delimiter=',')\n",
    "    \n",
    "    # save out sample size info   \n",
    "    if save_data_SZ == 1:                \n",
    "        name2give = newfolder + '/SpindleTR_SampleSize_N' + str(len(sbjlist)) + '.csv'\n",
    "        if os.path.isfile(name2give):\n",
    "            os.remove(name2give)\n",
    "        df_samples.to_csv(name2give, sep='\\t')  \n",
    "    \n",
    "if load_fmri_mat == 1: # just used existed matrix that already underwent the TR extraction steps.\n",
    "    print('  - Loading the big matrix, no hurries...')\n",
    "    fmri_all_selected = np.loadtxt(fmri_mat_name, delimiter=',')\n",
    "    print(' ')\n",
    "    print(80 * '+')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "#=================== [ do the clustering ] ====================#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if x_existed == 0: # no existed or need to be updated\n",
    "    X = fmri_all_selected\n",
    "    del fmri_all_selected\n",
    "\n",
    "for pca_on in range(0, 2):\n",
    "\n",
    "    # <PCA, dimensionality reduction>\n",
    "    if pca_on == 1:\n",
    "\n",
    "#         X = full_X\n",
    "        full_X = X\n",
    "\n",
    "        pca = PCA(n_components=50)\n",
    "#         pca = PCA(n_components=30)\n",
    "\n",
    "        X = pca.fit_transform(X)\n",
    "        # reduced_X.shape\n",
    "\n",
    "        newfolder =  newfolder + '_pca'\n",
    "        if os.path.isdir(newfolder):\n",
    "            print('< folder exists > : ' + newfolder)\n",
    "        else:\n",
    "            os.mkdir(newfolder)\n",
    "\n",
    "    # <pca help in finding proper seed to initiate> \n",
    "    # in this case the seeding of the centers is deterministic, hence we run the\n",
    "    # kmeans algorithm only once with n_init=1\n",
    "\n",
    "    # pca = PCA(n_components=n_digits).fit(data) # could get the center then initiate the seeding for clustering\n",
    "    # bench_k_means(KMeans(init=pca.components_, n_clusters=n_digits, n_init=1),\n",
    "    #               name=\"PCA-based\",\n",
    "    #               data=data)\n",
    "\n",
    "#     range_n_clusters = range(2, len(sbjlist)+1)\n",
    "    range_n_clusters = range(2, 10)\n",
    "\n",
    "    df_scores = pd.DataFrame(np.zeros((len(range_n_clusters), 3)),\n",
    "                       columns=['silhouette_avg', 'calinski_harabasz_score', 'davies_bouldin_score'])\n",
    "    df_scores['clusters']=range_n_clusters\n",
    "\n",
    "\n",
    "    for n_clusters in range_n_clusters:\n",
    "\n",
    "        print('  ')\n",
    "        print('  - solving for '+ str(n_clusters) + ' cluster plan...')\n",
    "\n",
    "        del clusterer, cluster_labels, silhouette_avg, ch_score, db_score  \n",
    "\n",
    "        # Create a subplot with 1 row and 2 columns\n",
    "        fig, (ax1) = plt.subplots(1, 1)\n",
    "        fig.set_size_inches(8, 7)\n",
    "\n",
    "        # The 1st subplot is the silhouette plot\n",
    "        # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "        # lie within [-0.1, 1]\n",
    "        ax1.set_xlim([-0.1, 1])\n",
    "        # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "        # plots of individual clusters, to demarcate them clearly.\n",
    "        ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "        # Initialize the clusterer with n_clusters value and a random generator\n",
    "        # seed of 10 for reproducibility.\n",
    "        clusterer = KMeans(init='k-means++', n_clusters=n_clusters, random_state=10).fit(X)\n",
    "    #     cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    #     clusterer = KMeans(n_clusters=3, random_state=1).fit(X)\n",
    "        cluster_labels = clusterer.labels_\n",
    "\n",
    "        # The silhouette_score gives the average value for all the samples.\n",
    "        # This gives a perspective into the density and separation of the formed\n",
    "        # clusters\n",
    "\n",
    "        print(40 * '-')\n",
    "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "        print(\"For n_clusters =\", n_clusters,\n",
    "              \", The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "        ch_score = calinski_harabasz_score(X, cluster_labels)\n",
    "        print(\"For n_clusters =\", n_clusters,\n",
    "              \", The calinski_harabasz_score is :\", ch_score)\n",
    "\n",
    "        db_score = davies_bouldin_score(X, cluster_labels)\n",
    "        print(\"For n_clusters =\", n_clusters,\n",
    "              \", The davies_bouldin_score is :\", db_score)\n",
    "\n",
    "        df_scores.silhouette_avg.iloc[range_n_clusters.index(n_clusters)] = silhouette_avg\n",
    "        df_scores.calinski_harabasz_score.iloc[range_n_clusters.index(n_clusters)] = ch_score\n",
    "        df_scores.davies_bouldin_score.iloc[range_n_clusters.index(n_clusters)] = db_score\n",
    "\n",
    "        print('  ')\n",
    "\n",
    "        # Compute the silhouette scores for each sample\n",
    "        sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "        y_lower = 10\n",
    "        for i in range(n_clusters):\n",
    "            # Aggregate the silhouette scores for samples belonging to\n",
    "            # cluster i, and sort them\n",
    "            ith_cluster_silhouette_values = \\\n",
    "                sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "            ith_cluster_silhouette_values.sort()\n",
    "\n",
    "            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "            y_upper = y_lower + size_cluster_i\n",
    "\n",
    "            color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "            ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                              0, ith_cluster_silhouette_values,\n",
    "                              facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "            # Label the silhouette plots with their cluster numbers at the middle\n",
    "            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "            # Compute the new y_lower for next plot\n",
    "            y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "        ax1.set_title(\"The silhouette plot for the various clusters\")\n",
    "        ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "        ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "        # The vertical line for average silhouette score of all the values\n",
    "        ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "        ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "\n",
    "\n",
    "    #     # 2nd Plot showing the actual clusters formed\n",
    "    #     colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    #     ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "    #                 c=colors, edgecolor='k')\n",
    "\n",
    "    #     # Labeling the clusters\n",
    "    #     centers = clusterer.cluster_centers_\n",
    "    #     # Draw white circles at cluster centers\n",
    "    #     ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "    #                 c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    #     for i, c in enumerate(centers):\n",
    "    #         ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "    #                     s=50, edgecolor='k')\n",
    "\n",
    "    #     ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    #     ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    #     ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "        plt.suptitle((\"Silhouette analysis for KMeans clustering, n_clusters = %d\" % n_clusters),\n",
    "                     fontsize=14, fontweight='bold')\n",
    "\n",
    "        plt.savefig(newfolder + '/silhouette_plot_cluster=%d.png' % n_clusters)\n",
    "        np.savetxt(newfolder + '/labels_cluster=%d.csv' % n_clusters, cluster_labels, delimiter=',')\n",
    "\n",
    "        for cc in range(clusterer.cluster_centers_.shape[0]):\n",
    "            curr_map = clusterer.cluster_centers_[cc, :]\n",
    "\n",
    "            if pca_on == 1:\n",
    "                curr_map = pca.inverse_transform(curr_map)\n",
    "\n",
    "\n",
    "            cc_img = spatial_mask.inverse_transform(curr_map)\n",
    "            cc_img.to_filename(newfolder + '/cluster=%d_cc%d_map.nii.gz' % (n_clusters, cc))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    name2give = newfolder + '/evaluation_scores_N' + str(len(sbjlist)) + '.csv'\n",
    "    if os.path.isfile(name2give):\n",
    "        os.remove(name2give)\n",
    "    df_scores.to_csv(name2give, sep='\\t')  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(curr_spinlde_mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CoRe_022_Day1_Night_01_spindles_TRnum_Pz_NREM23.txt',\n",
       " 'CoRe_022_Day1_Night_02_spindles_TRnum_Pz_NREM23.txt',\n",
       " 'CoRe_022_Day1_Night_01_spindles_TRnum_EEGmat_Pz_NREM23.txt',\n",
       " 'CoRe_022_Day1_Night_02_spindles_TRnum_EEGmat_Pz_NREM23.txt']"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_spinlde_mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sbjlist = [ \"CoRe_001\", \"CoRe_023\",  \"CoRe_079\", \"CoRe_107\",\n",
    "\"CoRe_082\", \"CoRe_100\", \"CoRe_087\", \"CoRe_192\", \"CoRe_195\",\n",
    "\"CoRe_235\", \"CoRe_267\", \"CoRe_237\", \"CoRe_296\",\n",
    "\"CoRe_011\", \"CoRe_022\", \"CoRe_050\", \"CoRe_054\", \"CoRe_162\", \"CoRe_155\", \"CoRe_094\"] \n",
    "df_samples = pd.DataFrame(np.zeros((len(sbjlist), 3)),\n",
    "                   columns=['total_TR', 'total_sleep_sess', 'specified_sleep_sess'])\n",
    "df_samples['sbjlist']=sbjlist\n",
    "df_samples.specified_sleep_sess+=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(369, 69881)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmri_all_selected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataload-fmri_all_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
